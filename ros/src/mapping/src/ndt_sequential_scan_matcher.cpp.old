// Copyright Â© 2018 Naoki Akai. All rights reserved.

#include <ros/ros.h>
#include <nav_msgs/Odometry.h>
#include <nav_msgs/Path.h>
#include <geometry_msgs/PoseWithCovarianceStamped.h>
#include <velodyne_pointcloud/point_types.h>
#include <velodyne_pointcloud/rawdata.h>
#include <tf/transform_broadcaster.h>
#include <tf/transform_listener.h>
#include <pcl/io/pcd_io.h>
#include <pcl/point_types.h>
#include <pcl/filters/approximate_voxel_grid.h>
#include <boost/thread/thread.hpp>

#include "ndt.h"

class SequentialScanMatcher
{
private:
	ros::NodeHandle nh;
	pcl::NormalDistributionsTransform<pcl::PointXYZ, pcl::PointXYZ> ndt;
	double transformation_epsilon, step_size, resolution;
	int maximum_iterations;
	std::string map_frame, base_link_frame, sensor_frame;
	tf::TransformListener tf_listener;
	Eigen::Matrix4f tf_base_link2sensor;
	std::string input_points_topic_name, input_odom_topic_name;
	ros::Subscriber points_sub, odom_sub;
	ros::Publisher pose_pub, robot_trajectory_pub, sensor_trajectory_pub;
	geometry_msgs::PoseStamped current_robot_pose, current_sensor_pose;
	geometry_msgs::PoseWithCovarianceStamped robot_pose, sensor_pose;
	nav_msgs::Path robot_trajectory, sensor_trajectory;
	double mapping_interval_dist, voxel_grid_filter_size, covariance_coefficient;
	double start_x, start_y, start_z, start_roll, start_pitch, start_yaw;

public:
	SequentialScanMatcher();
	void quaternion2rpy(double qx, double qy, double qz, double qw, double* r, double* p, double* y);
	void rpy2quaternion(double r, double p, double y, double* qx, double* qy, double* qz, double* qw);
	void compute_current_robot_and_sensor_poses(void);
	void points_callback(const pcl::PointCloud<pcl::PointXYZ>::ConstPtr& msg);
	void odom_callback(const nav_msgs::Odometry::ConstPtr& msg);
};

SequentialScanMatcher::SequentialScanMatcher():
	nh("~"),
	transformation_epsilon(0.01),
	step_size(0.1),
	resolution(1.0),
	maximum_iterations(35),
	map_frame("/map"),
	base_link_frame("/base_link"),
	sensor_frame("/velodyne"),
	input_points_topic_name("/velodyne_points"),
	input_odom_topic_name("/odom"),
	mapping_interval_dist(1.0),
	voxel_grid_filter_size(1.0),
	covariance_coefficient(1000000.0),
	tf_listener()
{
	// read parameters
	nh.param("/ndt_sequential_scan_matcher/transformation_epsilon", transformation_epsilon, transformation_epsilon);
	nh.param("/ndt_sequential_scan_matcher/step_size", step_size, step_size);
	nh.param("/ndt_sequential_scan_matcher/resolution", resolution, resolution);
	nh.param("/ndt_sequential_scan_matcher/maximum_iterations", maximum_iterations, maximum_iterations);
	nh.param("/ndt_sequential_scan_matcher/map_frame", map_frame, map_frame);
	nh.param("/ndt_sequential_scan_matcher/base_link_frame", base_link_frame, base_link_frame);
	nh.param("/ndt_sequential_scan_matcher/sensor_frame", sensor_frame, sensor_frame);
	nh.param("/ndt_sequential_scan_matcher/input_points_topic_name", input_points_topic_name, input_points_topic_name);
	nh.param("/ndt_sequential_scan_matcher/input_odom_topic_name", input_odom_topic_name, input_odom_topic_name);
	nh.param("/ndt_sequential_scan_matcher/mapping_interval_dist", mapping_interval_dist, mapping_interval_dist);
	nh.param("/ndt_sequential_scan_matcher/voxel_grid_filter_size", voxel_grid_filter_size, voxel_grid_filter_size);
	nh.param("/ndt_sequential_scan_matcher/covariance_coefficient", covariance_coefficient, covariance_coefficient);
	// subscriber
	points_sub = nh.subscribe(input_points_topic_name, 10, &SequentialScanMatcher::points_callback, this);
	odom_sub = nh.subscribe(input_odom_topic_name, 10, &SequentialScanMatcher::odom_callback, this);
	// publisher
	pose_pub = nh.advertise<geometry_msgs::PoseWithCovarianceStamped>("/ndt_ssm_sensor_pose", 1);
	robot_trajectory_pub = nh.advertise<nav_msgs::Path>("/ndt_ssm_robot_trajectory", 1, true);
	sensor_trajectory_pub = nh.advertise<nav_msgs::Path>("/ndt_ssm_sensor_trajectory", 1, true);
	// initialization of tf
	tf::StampedTransform base_link2sensor;
	ros::Rate loop_rate(10);
	while (ros::ok())
	{
		try
		{
			ros::Time now = ros::Time::now();
			tf_listener.waitForTransform(base_link_frame, sensor_frame, now, ros::Duration(1.0));
			tf_listener.lookupTransform(base_link_frame, sensor_frame, now, base_link2sensor);
			break;
		}
		catch (tf::TransformException ex)
		{
			ROS_ERROR("%s", ex.what());
			loop_rate.sleep();
		}
	}
	Eigen::Translation3f b2s_trans(base_link2sensor.getOrigin().x(), base_link2sensor.getOrigin().y(), base_link2sensor.getOrigin().z());
	double roll, pitch, yaw;
	quaternion2rpy(base_link2sensor.getRotation().x(),
		base_link2sensor.getRotation().y(),
		base_link2sensor.getRotation().z(),
		base_link2sensor.getRotation().w(),
		&roll, &pitch, &yaw);
	Eigen::AngleAxisf b2s_rot_x(roll, Eigen::Vector3f::UnitX());
	Eigen::AngleAxisf b2s_rot_y(pitch, Eigen::Vector3f::UnitY());
	Eigen::AngleAxisf b2s_rot_z(yaw, Eigen::Vector3f::UnitZ());
	tf_base_link2sensor = (b2s_trans * b2s_rot_x * b2s_rot_y * b2s_rot_z).matrix();
	// set initial robot poses
	double qx, qy, qz, qw;
	rpy2quaternion(0.0, 0.0, 0.0, &qx, &qy, &qz, &qw);
	robot_pose.pose.pose.position.x = 0.0;
	robot_pose.pose.pose.position.y = 0.0;
	robot_pose.pose.pose.position.z = 0.0;
	robot_pose.pose.pose.orientation.x = qx;
	robot_pose.pose.pose.orientation.y = qy;
	robot_pose.pose.pose.orientation.z = qz;
	robot_pose.pose.pose.orientation.w = qw;
	current_robot_pose.pose = robot_pose.pose.pose;
	// set initial sensor poses
	tf::Matrix3x3 mat_s;
	mat_s.setValue(static_cast<double>(tf_base_link2sensor(0, 0)), static_cast<double>(tf_base_link2sensor(0, 1)), static_cast<double>(tf_base_link2sensor(0, 2)),
		static_cast<double>(tf_base_link2sensor(1, 0)), static_cast<double>(tf_base_link2sensor(1, 1)), static_cast<double>(tf_base_link2sensor(1, 2)),
		static_cast<double>(tf_base_link2sensor(2, 0)), static_cast<double>(tf_base_link2sensor(2, 1)), static_cast<double>(tf_base_link2sensor(2, 2)));
	mat_s.getRPY(roll, pitch, yaw);
	rpy2quaternion(roll, pitch, yaw, &qx, &qy, &qz, &qw);
	sensor_pose.header.stamp = ros::Time::now();
	sensor_pose.pose.pose.position.x = tf_base_link2sensor(0, 3);
	sensor_pose.pose.pose.position.y = tf_base_link2sensor(1, 3);
	sensor_pose.pose.pose.position.z = tf_base_link2sensor(2, 3);
	sensor_pose.pose.pose.orientation.x = qx;
	sensor_pose.pose.pose.orientation.y = qy;
	sensor_pose.pose.pose.orientation.z = qz;
	sensor_pose.pose.pose.orientation.w = qw;
	current_sensor_pose.pose = sensor_pose.pose.pose;
	// set frame of messages
	robot_pose.header.frame_id = current_robot_pose.header.frame_id = robot_trajectory.header.frame_id = map_frame;
	sensor_pose.header.frame_id = current_sensor_pose.header.frame_id = sensor_trajectory.header.frame_id = map_frame;
	// add initial pose to trajecotry buffer
	robot_trajectory.poses.push_back(current_robot_pose);
	sensor_trajectory.poses.push_back(current_sensor_pose);
	// set ndt parameters
	ndt.setTransformationEpsilon(transformation_epsilon);
	ndt.setStepSize(step_size);
	ndt.setResolution(resolution);
	ndt.setMaximumIterations(maximum_iterations);
	printf("finish initialization\n");
	// main loop
	ros::spin();
}

void SequentialScanMatcher::quaternion2rpy(double qx, double qy, double qz, double qw, double* r, double* p, double* y)
{
	tf::Quaternion q(qx, qy, qz, qw);
	double roll, pitch, yaw;
	tf::Matrix3x3 m(q);
	m.getRPY(roll, pitch, yaw);
	*r = roll;
	*p = pitch;
	*y = yaw;
}

void SequentialScanMatcher::rpy2quaternion(double r, double p, double y, double* qx, double* qy, double* qz, double* qw)
{
	tf::Transform tf;
	tf::Quaternion q;
	tf.setOrigin(tf::Vector3(0.0, 0.0, 0.0));
	q.setRPY(r, p, y);
	tf.setRotation(q);
	*qx = tf.getRotation().x();
	*qy = tf.getRotation().y();
	*qz = tf.getRotation().z();
	*qw = tf.getRotation().w();
}

void SequentialScanMatcher::compute_current_robot_and_sensor_poses(void)
{
	// create transformation matrix from map to last pose of robot trajectory
	int i = (int)robot_trajectory.poses.size() - 1;
	Eigen::Translation3f m2lp_trans(robot_trajectory.poses[i].pose.position.x,
		robot_trajectory.poses[i].pose.position.y,
		robot_trajectory.poses[i].pose.position.z);
	double roll, pitch, yaw;
	quaternion2rpy(robot_trajectory.poses[i].pose.orientation.x,
		robot_trajectory.poses[i].pose.orientation.y,
		robot_trajectory.poses[i].pose.orientation.z,
		robot_trajectory.poses[i].pose.orientation.w,
		&roll, &pitch, &yaw);
	Eigen::AngleAxisf m2lp_rot_x(roll, Eigen::Vector3f::UnitX());
	Eigen::AngleAxisf m2lp_rot_y(pitch, Eigen::Vector3f::UnitY());
	Eigen::AngleAxisf m2lp_rot_z(yaw, Eigen::Vector3f::UnitZ());
	Eigen::Matrix4f tf_map2last_pose = (m2lp_trans * m2lp_rot_z * m2lp_rot_y * m2lp_rot_x).matrix();
	// create transformation matrix from last pose of robot trajectory to current robot pose
	Eigen::Translation3f lp2cp_trans(robot_pose.pose.pose.position.x, robot_pose.pose.pose.position.y, robot_pose.pose.pose.position.z);
	quaternion2rpy(robot_pose.pose.pose.orientation.x,
		robot_pose.pose.pose.orientation.y,
		robot_pose.pose.pose.orientation.z,
		robot_pose.pose.pose.orientation.w,
		&roll, &pitch, &yaw);
	Eigen::AngleAxisf lp2cp_rot_x(roll, Eigen::Vector3f::UnitX());
	Eigen::AngleAxisf lp2cp_rot_y(pitch, Eigen::Vector3f::UnitY());
	Eigen::AngleAxisf lp2cp_rot_z(yaw, Eigen::Vector3f::UnitZ());
	Eigen::Matrix4f tf_last_pose2current_pose = (lp2cp_trans * lp2cp_rot_z * lp2cp_rot_y * lp2cp_rot_x).matrix();
	// compute current robot pose in map frame
	Eigen::Matrix4f cpose_mat = tf_map2last_pose * tf_last_pose2current_pose;
	tf::Matrix3x3 mat_c;
	mat_c.setValue(static_cast<double>(cpose_mat(0, 0)), static_cast<double>(cpose_mat(0, 1)), static_cast<double>(cpose_mat(0, 2)),
		static_cast<double>(cpose_mat(1, 0)), static_cast<double>(cpose_mat(1, 1)), static_cast<double>(cpose_mat(1, 2)),
		static_cast<double>(cpose_mat(2, 0)), static_cast<double>(cpose_mat(2, 1)), static_cast<double>(cpose_mat(2, 2)));
	double qx, qy, qz, qw;
	mat_c.getRPY(roll, pitch, yaw);
	rpy2quaternion(roll, pitch, yaw, &qx, &qy, &qz, &qw);
	current_robot_pose.header.stamp = ros::Time::now();
	current_robot_pose.pose.position.x = cpose_mat(0, 3);
	current_robot_pose.pose.position.y = cpose_mat(1, 3);
	current_robot_pose.pose.position.z = cpose_mat(2, 3);
	current_robot_pose.pose.orientation.x = qx;
	current_robot_pose.pose.orientation.y = qy;
	current_robot_pose.pose.orientation.z = qz;
	current_robot_pose.pose.orientation.w = qw;
	// compute current sensor pose in map frame
	Eigen::Matrix4f spose_mat = cpose_mat * tf_base_link2sensor;
	tf::Matrix3x3 mat_s;
	mat_s.setValue(static_cast<double>(spose_mat(0, 0)), static_cast<double>(spose_mat(0, 1)), static_cast<double>(spose_mat(0, 2)),
		static_cast<double>(spose_mat(1, 0)), static_cast<double>(spose_mat(1, 1)), static_cast<double>(spose_mat(1, 2)),
		static_cast<double>(spose_mat(2, 0)), static_cast<double>(spose_mat(2, 1)), static_cast<double>(spose_mat(2, 2)));
	mat_s.getRPY(roll, pitch, yaw);
	rpy2quaternion(roll, pitch, yaw, &qx, &qy, &qz, &qw);
	current_sensor_pose.header.stamp = ros::Time::now();
	current_sensor_pose.pose.position.x = spose_mat(0, 3);
	current_sensor_pose.pose.position.y = spose_mat(1, 3);
	current_sensor_pose.pose.position.z = spose_mat(2, 3);
	current_sensor_pose.pose.orientation.x = qx;
	current_sensor_pose.pose.orientation.y = qy;
	current_sensor_pose.pose.orientation.z = qz;
	current_sensor_pose.pose.orientation.w = qw;
	// broadcast tf
	tf::Transform tf;
	tf::Quaternion q;
	static tf::TransformBroadcaster br;
	tf.setOrigin(tf::Vector3(current_robot_pose.pose.position.x, current_robot_pose.pose.position.y, current_robot_pose.pose.position.z));
	q.setRPY(roll, pitch, yaw);
	tf.setRotation(q);
	br.sendTransform(tf::StampedTransform(tf, ros::Time::now(), map_frame, base_link_frame));
	printf("current pose position: x = %.3lf, y = %.3lf, z = %lf\n", current_robot_pose.pose.position.x, current_robot_pose.pose.position.y, current_robot_pose.pose.position.z);
	printf("current pose angle: roll = %.3lf, pitch = %.3lf, yaw = %.3lf\n", roll * 180.0 / M_PI, pitch * 180.0 / M_PI, yaw * 180.0 / M_PI);
}

void SequentialScanMatcher::points_callback(const pcl::PointCloud<pcl::PointXYZ>::ConstPtr& msg)
{
	static bool is_first = true;
	static geometry_msgs::PoseWithCovarianceStamped prev_robot_pose;
	if (is_first)
	{
		ndt.setInputTarget(msg);
		prev_robot_pose = robot_pose;
		is_first = false;
		return;
	}
	// compute moving amout
	double dx = robot_pose.pose.pose.position.x - prev_robot_pose.pose.pose.position.x;
	double dy = robot_pose.pose.pose.position.y - prev_robot_pose.pose.pose.position.y;
	double dl = sqrt(dx * dx + dy * dy);
	if (dl < mapping_interval_dist)
		return;
	double roll, pitch, yaw, qx, qy, qz, qw;
	// filtering input data and setting filtered data as input source
	pcl::PointCloud<pcl::PointXYZ>::Ptr filtered_cloud(new pcl::PointCloud<pcl::PointXYZ>);
	pcl::ApproximateVoxelGrid<pcl::PointXYZ> approximate_voxel_filter;
	approximate_voxel_filter.setLeafSize(voxel_grid_filter_size, voxel_grid_filter_size, voxel_grid_filter_size);
	approximate_voxel_filter.setInputCloud(msg);
	approximate_voxel_filter.filter(*filtered_cloud);
	ndt.setInputSource(filtered_cloud);
	// compute current sensor pose in coordinates that uses previsou sensor pose as origin (current sensor pose is used as initial pose of scan matching)
	// local map (previous robot pose) to base_link
	Eigen::Translation3f lm2bl_trans(robot_pose.pose.pose.position.x, robot_pose.pose.pose.position.y, robot_pose.pose.pose.position.z);
	quaternion2rpy(robot_pose.pose.pose.orientation.x,
		robot_pose.pose.pose.orientation.y,
		robot_pose.pose.pose.orientation.z,
		robot_pose.pose.pose.orientation.w,
		&roll, &pitch, &yaw);
	Eigen::AngleAxisf lm2bl_rot_x(roll, Eigen::Vector3f::UnitX());
	Eigen::AngleAxisf lm2bl_rot_y(pitch, Eigen::Vector3f::UnitY());
	Eigen::AngleAxisf lm2bl_rot_z(yaw, Eigen::Vector3f::UnitZ());
	Eigen::Matrix4f tf_local_map2base_link = (lm2bl_trans * lm2bl_rot_x * lm2bl_rot_y * lm2bl_rot_z).matrix();
	// map to sensor
	Eigen::Matrix4f tf_local_map2sensor = tf_local_map2base_link * tf_base_link2sensor;
	// local sensor frame (previous sensor pose) to current sensor pose
	Eigen::Matrix4f tf_local_sensor2current_sensor = tf_local_map2sensor * tf_base_link2sensor.inverse();
	tf::Matrix3x3 mat_cs;
	mat_cs.setValue(static_cast<double>(tf_local_sensor2current_sensor(0, 0)), static_cast<double>(tf_local_sensor2current_sensor(0, 1)), static_cast<double>(tf_local_sensor2current_sensor(0, 2)),
		static_cast<double>(tf_local_sensor2current_sensor(1, 0)), static_cast<double>(tf_local_sensor2current_sensor(1, 1)), static_cast<double>(tf_local_sensor2current_sensor(1, 2)),
		static_cast<double>(tf_local_sensor2current_sensor(2, 0)), static_cast<double>(tf_local_sensor2current_sensor(2, 1)), static_cast<double>(tf_local_sensor2current_sensor(2, 2)));
	mat_cs.getRPY(roll, pitch, yaw);
	// set initial alignment (node this alignment is in previous sensor frame)
	Eigen::AngleAxisf init_rotation_roll(roll, Eigen::Vector3f::UnitX());
	Eigen::AngleAxisf init_rotation_pitch(pitch, Eigen::Vector3f::UnitY());
	Eigen::AngleAxisf init_rotation_yaw(yaw, Eigen::Vector3f::UnitZ());
	Eigen::Translation3f init_translation(tf_local_sensor2current_sensor(0, 3), tf_local_sensor2current_sensor(1, 3), tf_local_sensor2current_sensor(2, 3));
	Eigen::Matrix4f init_guess = (init_translation * init_rotation_yaw * init_rotation_pitch * init_rotation_roll).matrix();
	// perform scan matching
	pcl::PointCloud<pcl::PointXYZ>::Ptr output_cloud(new pcl::PointCloud<pcl::PointXYZ>);
	ndt.align(*output_cloud, init_guess);
	Eigen::Matrix4f sensor_mat_psf = ndt.getFinalTransformation(); // optimized sensor pose in previous sensor frame (it can be used as constraint for graph optimization)
	// map to optimized sensor pose
	Eigen::Matrix4f sensor_mat = tf_base_link2sensor * sensor_mat_psf; // opimized sensor pose in local map frame
	Eigen::Matrix4f base_link_mat = sensor_mat * tf_base_link2sensor.inverse(); // opimized base link (robot pose) in local map frame
	int iteration = ndt.getFinalNumIteration();
	double fitness_score = ndt.getFitnessScore();
	double trans_probability = ndt.getTransformationProbability();
	int scan_points_num = (int)filtered_cloud->size();
	Eigen::Matrix<double, 6, 6> covariance = (covariance_coefficient * fitness_score) / (double)scan_points_num * ndt.hessian_.inverse();
	// update sensor pose
	// origin of this sensor pose is previous robot pose, thus it must be transformed in previous sensor pose frame in order to use it as constraint between sequential sensor poses
	tf::Matrix3x3 mat_s;
	mat_s.setValue(static_cast<double>(sensor_mat(0, 0)), static_cast<double>(sensor_mat(0, 1)), static_cast<double>(sensor_mat(0, 2)),
		static_cast<double>(sensor_mat(1, 0)), static_cast<double>(sensor_mat(1, 1)), static_cast<double>(sensor_mat(1, 2)),
		static_cast<double>(sensor_mat(2, 0)), static_cast<double>(sensor_mat(2, 1)), static_cast<double>(sensor_mat(2, 2)));
	mat_s.getRPY(roll, pitch, yaw);
	rpy2quaternion(roll, pitch, yaw, &qx, &qy, &qz, &qw);
	sensor_pose.pose.pose.position.x = sensor_mat(0, 3);
	sensor_pose.pose.pose.position.y = sensor_mat(1, 3);
	sensor_pose.pose.pose.position.z = sensor_mat(2, 3);
	sensor_pose.pose.pose.orientation.x = qx;
	sensor_pose.pose.pose.orientation.y = qy;
	sensor_pose.pose.pose.orientation.z = qz;
	sensor_pose.pose.pose.orientation.w = qw;
	for (int j = 0; j < 6; j++)
	{
		for (int i = 0; i < 6; i++)
			sensor_pose.pose.covariance[i * 6 + j] = covariance(i, j);
	}
	// update robot pose (because robot pose won't be used as constraint for graph optimization, its covariance is not computed)
	tf::Matrix3x3 mat_b;
	mat_b.setValue(static_cast<double>(base_link_mat(0, 0)), static_cast<double>(base_link_mat(0, 1)), static_cast<double>(base_link_mat(0, 2)),
		static_cast<double>(base_link_mat(1, 0)), static_cast<double>(base_link_mat(1, 1)), static_cast<double>(base_link_mat(1, 2)),
		static_cast<double>(base_link_mat(2, 0)), static_cast<double>(base_link_mat(2, 1)), static_cast<double>(base_link_mat(2, 2)));
	mat_b.getRPY(roll, pitch, yaw);
	rpy2quaternion(roll, pitch, yaw, &qx, &qy, &qz, &qw);
	robot_pose.pose.pose.position.x = base_link_mat(0, 3);
	robot_pose.pose.pose.position.y = base_link_mat(1, 3);
	robot_pose.pose.pose.position.z = base_link_mat(2, 3);
	robot_pose.pose.pose.orientation.x = qx;
	robot_pose.pose.pose.orientation.y = qy;
	robot_pose.pose.pose.orientation.z = qz;
	robot_pose.pose.pose.orientation.w = qw;
	// update and publish robot trajectory and sensor pose
	compute_current_robot_and_sensor_poses();
	robot_trajectory.poses.push_back(current_robot_pose);
	sensor_trajectory.poses.push_back(current_sensor_pose);
	pose_pub.publish(sensor_pose);
	robot_trajectory_pub.publish(robot_trajectory);
	sensor_trajectory_pub.publish(sensor_trajectory);
	// reset robot pose to zero
	rpy2quaternion(0.0, 0.0, 0.0, &qx, &qy, &qz, &qw);
	robot_pose.pose.pose.position.x = 0.0;
	robot_pose.pose.pose.position.y = 0.0;
	robot_pose.pose.pose.position.z = 0.0;
	robot_pose.pose.pose.orientation.x = qx;
	robot_pose.pose.pose.orientation.y = qy;
	robot_pose.pose.pose.orientation.z = qz;
	robot_pose.pose.pose.orientation.w = qw;
	// record current data as previous data
	ndt.setInputTarget(msg);
	prev_robot_pose = robot_pose;
	// print info
	printf("iteration = %d\n", iteration);
	printf("fitness_score = %lf\n", fitness_score);
	printf("trans_probability = %lf\n", trans_probability);
	printf("scan_points_num = %d\n", scan_points_num);
	std::cout << "covariance\n" << covariance << std::endl;
//	std::cout << "init_guess\n" << init_guess << std::endl;
//	std::cout << "base_link_mat\n" << base_link_mat << std::endl;
//	std::cout << "sensor_mat\n" << sensor_mat << std::endl;
	printf("\n");
}

void SequentialScanMatcher::odom_callback(const nav_msgs::Odometry::ConstPtr& msg)
{
	static bool is_first = true;
	static double prev_yaw;
	static nav_msgs::Odometry prev_odom;
	if (is_first)
	{
		prev_odom = *msg;
		double roll, pitch;
		quaternion2rpy(msg->pose.pose.orientation.x,
			msg->pose.pose.orientation.y,
			msg->pose.pose.orientation.z,
			msg->pose.pose.orientation.w,
			&roll, &pitch, &prev_yaw);
		is_first = false;
		return;
	}
	// compute moving amount
	double roll, pitch, yaw;
	quaternion2rpy(msg->pose.pose.orientation.x,
		msg->pose.pose.orientation.y,
		msg->pose.pose.orientation.z,
		msg->pose.pose.orientation.w,
		&roll, &pitch, &yaw);
	double dx = msg->pose.pose.position.x - prev_odom.pose.pose.position.x;
	double dy = msg->pose.pose.position.y - prev_odom.pose.pose.position.y;
	double dyaw = yaw - prev_yaw;
	prev_yaw = yaw;
	if (dyaw < -M_PI)
		dyaw += 2.0 * M_PI;
	if (dyaw > M_PI)
		dyaw -= 2.0 * M_PI;
	double d_rot1 = atan2(dy, dx) - prev_yaw;
	if (d_rot1 < -M_PI)
		d_rot1 += 2.0 * M_PI;
	if (d_rot1 > M_PI)
		d_rot1 -= 2.0 * M_PI;
	double d_trans = sqrt(dx * dx + dy * dy);
	if (msg->twist.twist.linear.x < 0.0)
		d_trans *= -1.0;
	double d_rot2 = dyaw - d_rot1;
	if (d_rot2 < -M_PI)
		d_rot2 += 2.0 * M_PI;
	if (d_rot2 > M_PI)
		d_rot2 -= 2.0 * M_PI;
	// update robot pose
	quaternion2rpy(robot_pose.pose.pose.orientation.x,
		robot_pose.pose.pose.orientation.y,
		robot_pose.pose.pose.orientation.z,
		robot_pose.pose.pose.orientation.w,
		&roll, &pitch, &yaw);
	robot_pose.pose.pose.position.x += d_trans * cos(yaw + d_rot1);
	robot_pose.pose.pose.position.y += d_trans * sin(yaw + d_rot1);
	yaw += d_rot1 + d_rot2;
	if (yaw < -M_PI)
		yaw += 2.0 * M_PI;
	if (yaw > M_PI)
		yaw -= 2.0 * M_PI;
	double qx, qy, qz, qw;
	rpy2quaternion(roll, pitch, yaw, &qx, &qy, &qz, &qw);
	robot_pose.pose.pose.orientation.x = qx;
	robot_pose.pose.pose.orientation.y = qy;
	robot_pose.pose.pose.orientation.z = qz;
	robot_pose.pose.pose.orientation.w = qw;
	// record current data as previous data
	prev_odom = *msg;
}

int main(int argc, char** argv)
{
	ros::init(argc, argv, "ndt_sequential_scan_matcher");
	SequentialScanMatcher node;
	return 0;
}
